{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Deep Learning: Neural Style Transfer</h1>\n\n<h4>\n* Implement the neural style transfer algorithm\n    \n</h4>","metadata":{}},{"cell_type":"code","source":"!pip install keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.preprocessing.image as process_im\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications import vgg19\nfrom tensorflow.keras.models import Model\nfrom tensorflow.python.keras import models \nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import backend as K\nimport functools\nimport IPython.display","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style_path = '../input/images/ac944d61e6274435b49568c2a1564c57.jpeg'\ncontent_path = '../input/images/DSC_0006.JPG'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to load images and return numpy array</h2>","metadata":{}},{"cell_type":"code","source":"def load_file(image_path):\n    image =  Image.open(image_path)\n    max_dim=512\n    factor=max_dim/max(image.size)\n    image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n    im_array = process_im.img_to_array(image)\n    im_array = np.expand_dims(im_array,axis=0) #adding extra axis to the array as to generate a \n                                               #batch of single image \n    \n    return im_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to plot image</h2>","metadata":{}},{"cell_type":"code","source":"def show_im(img,title=None):\n    img=np.squeeze(img,axis=0) #squeeze array to drop batch axis\n    plt.imshow(np.uint8(img))\n    if title is None:\n        pass\n    else:\n        plt.title(title)\n    plt.imshow(np.uint8(img))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Plot Image</h2>","metadata":{}},{"cell_type":"code","source":"content = load_file(content_path)\nstyle = load_file(style_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncontent = load_file(content_path)\nstyle = load_file(style_path)\nplt.subplot(1,2,1)\nshow_im(content,'Content Image')\nplt.subplot(1,2,2)\nshow_im(style,'Style Image')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to process image for input to vgg19 model</h2>","metadata":{}},{"cell_type":"code","source":"def img_preprocess(img_path):\n    image=load_file(img_path)\n    img=tf.keras.applications.vgg19.preprocess_input(image)\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to deprocess image</h2>\n\n**VGG networks are trained on image with each channel normalized by mean = [103.939, 116.779, 123.68]and with channels BGR.**","metadata":{}},{"cell_type":"code","source":"def deprocess_img(processed_img):\n  x = processed_img.copy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n  \n  \n  # perform the inverse of the preprocessing step\n  x[:, :, 0] += 103.939\n  x[:, :, 1] += 116.779\n  x[:, :, 2] += 123.68\n  x = x[:, :, ::-1] # converting BGR to RGB channel\n\n  x = np.clip(x, 0, 255).astype('uint8')\n  return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im=img_preprocess(content_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Get necessary layers from vgg19 model</h2>","metadata":{}},{"cell_type":"code","source":"content_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\nnumber_content=len(content_layers)\nnumber_style =len(style_layers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to get vgg19 model with pretrained weights</h2>","metadata":{}},{"cell_type":"code","source":"def get_model():\n    \n    vgg=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n    vgg.trainable=False\n    content_output=[vgg.get_layer(layer).output for layer in content_layers]\n    style_output=[vgg.get_layer(layer).output for layer in style_layers]\n    model_output= style_output+content_output\n    return models.Model(vgg.input,model_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Model architecture</h2>","metadata":{}},{"cell_type":"code","source":"model=get_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Loss Functions</h1>\n<h3>Neural style transfer is done by defining two loss functions that try to minimise the differences between a content image, a style image and a generated image. Take the base input image, the content image and the style image that needs to be matched and transform the base input image by minimizing the content and style distances (losses) with backpropagation, creating an image that matches the content of the content image and the style of the style image.\n\nThe content loss function ensures that the activations of the higher layers are similar between the content image and the generated image. The style loss function makes sure that the correlation of activations in all the layers are similar between the style image and the generated image.\n\n**Define content loss**\nEssentially content loss captures the root mean squared error between the activations produced by the generated image and the content image.</h3>","metadata":{}},{"cell_type":"code","source":"def get_content_loss(noise,target):\n    loss = tf.reduce_mean(tf.square(noise-target))\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>\n    \n****Define style loss****\n    \nThe goal is to compute a style matrix for the generated image and the style image. Then the style loss is defined as the root mean square difference between the two style matrices. Style information is measured as the amount of correlation present between features maps in a given layer. Next, a loss is defined as the difference of correlation present between the feature maps computed by the generated image and the style image. The gram matrix is used to find the correlation between the feature maps of a convolution layer.\n</h2>","metadata":{}},{"cell_type":"code","source":"def gram_matrix(tensor):\n    channels=int(tensor.shape[-1])\n    vector=tf.reshape(tensor,[-1,channels])\n    n=tf.shape(vector)[0]\n    gram_matrix=tf.matmul(vector,vector,transpose_a=True)\n    return gram_matrix/tf.cast(n,tf.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_style_loss(noise,target):\n    gram_noise=gram_matrix(noise)\n    #gram_target=gram_matrix(target)\n    loss=tf.reduce_mean(tf.square(target-gram_noise))\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(model,content_path,style_path):\n    content_img=img_preprocess(content_path)\n    style_image=img_preprocess(style_path)\n    \n    content_output=model(content_img)\n    style_output=model(style_image)\n    \n    content_feature = [layer[0] for layer in content_output[number_style:]]\n    style_feature = [layer[0] for layer in style_output[:number_style]]\n    return content_feature,style_feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to compute total loss</h2>","metadata":{}},{"cell_type":"code","source":"def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n    style_weight,content_weight = loss_weights #style weight and content weight are user given parameters\n                                               #that define what percentage of content and/or style will be preserved in the generated image\n    \n    output=model(image)\n    content_loss=0\n    style_loss=0\n    \n    noise_style_features = output[:number_style]\n    noise_content_feature = output[number_style:]\n    \n    weight_per_layer = 1.0/float(number_style)\n    for a,b in zip(gram_style_features,noise_style_features):\n        style_loss+=weight_per_layer*get_style_loss(b[0],a)\n        \n    \n    weight_per_layer =1.0/ float(number_content)\n    for a,b in zip(noise_content_feature,content_features):\n        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n        \n    style_loss *= style_weight\n    content_loss *= content_weight\n    \n    total_loss = content_loss + style_loss\n    \n    \n    return total_loss,style_loss,content_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Define function to calculate gradient</h2>","metadata":{}},{"cell_type":"code","source":"def compute_grads(dictionary):\n    with tf.GradientTape() as tape:\n        all_loss=compute_loss(**dictionary)\n        \n    total_loss=all_loss[0]\n    return tape.gradient(total_loss,dictionary['image']),all_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_style_transfer(content_path,style_path,epochs=500,content_weight=1e3, style_weight=1e-2):\n    \n    model=get_model()\n    \n    for layer in model.layers:\n        layer.trainable = False\n        \n    content_feature,style_feature = get_features(model,content_path,style_path)\n    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n    \n    noise = img_preprocess(content_path)\n    noise=tf.Variable(noise,dtype=tf.float32)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n    \n    best_loss,best_img=float('inf'),None\n    \n    loss_weights = (style_weight, content_weight)\n    dictionary={'model':model,\n              'loss_weights':loss_weights,\n              'image':noise,\n              'gram_style_features':style_gram_matrix,\n              'content_features':content_feature}\n    \n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n  \n    imgs = []\n    for i in range(epochs):\n        grad,all_loss=compute_grads(dictionary)\n        total_loss,style_loss,content_loss=all_loss\n        optimizer.apply_gradients([(grad,noise)])\n        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n        noise.assign(clipped)\n        \n        if total_loss<best_loss:\n            best_loss = total_loss\n            best_img = deprocess_img(noise.numpy())\n            \n         #for visualization   \n            \n        if i%5==0:\n            plot_img = noise.numpy()\n            plot_img = deprocess_img(plot_img)\n            imgs.append(plot_img)\n            IPython.display.clear_output(wait=True)\n            IPython.display.display_png(Image.fromarray(plot_img))\n            print('Epoch: {}'.format(i))        \n            print('Total loss: {:.4e}, ' \n              'style loss: {:.4e}, '\n              'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n    \n    IPython.display.clear_output(wait=True)\n    \n    \n    return best_img,best_loss,imgs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best, best_loss,image = run_style_transfer(content_path, style_path, epochs=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}